{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "ed7cbdb76f029e40b2938ce9386b50261706ee7945092ce446037e65149312dc"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Notebook for learning deep learning methodolgies and algorithms that help in fulfilling them"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries (commonly used libraries)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "source": [
    "Forward propagation in the neural networks\n",
    "\n",
    "Dot products of the weights of each node transition to the node values will give the next node value\n",
    "\n",
    "Carried out one data point at a time, each data point is carried out till the output layer\n",
    "\n",
    "Prediction of the data point is given at the output layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward propagation algorithm\n",
    "input_data = np.array([2,3])\n",
    "weights = {\"node_0\": np.array([1,1]),\n",
    "            \"node_1\": np.array([-1,1]),\n",
    "            \"output\": np.array([2,-1])}\n",
    "node_0_value = (input_data * weights[\"node_0\"]).sum()\n",
    "node_1_value = (input_data * weights[\"node_1\"]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[5 1]\n"
     ]
    }
   ],
   "source": [
    "hidden_layers_values = np.array([node_0_value, node_1_value])\n",
    "print(hidden_layers_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "output = (hidden_layers_values * weights[\"output\"]).sum()\n",
    "print(output)"
   ]
  },
  {
   "source": [
    "Activation Function\n",
    "\n",
    "Functions used to consider the non-linearity of the data points \n",
    "\n",
    "Applied to the hidden layer node values and is used on the node inputs caculated from the input layer and the weights applied."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9901095378334199\n"
     ]
    }
   ],
   "source": [
    "input_data = np.array([-1,2])\n",
    "\n",
    "weights = { \"node_0\": np.array([3,3]),\n",
    "            \"node_1\": np.array([1,5]),\n",
    "            \"output\": np.array([2,-1])}\n",
    "\n",
    "node_0_input = (input_data * weights[\"node_0\"]).sum()\n",
    "node_0_output = np.tanh(node_0_input)\n",
    "\n",
    "node_1_input = (input_data * weights[\"node_1\"]).sum()\n",
    "node_1_output = np.tanh(node_1_input)\n",
    "\n",
    "hidden_layers_output = np.array([node_0_output, node_1_output])\n",
    "output = (hidden_layers_output * weights[\"output\"]).sum()\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "source": [
    "Rectified Linear Activation Function (ReLU)\n",
    "\n",
    "- Shown to lead to very high-performance netowrks\n",
    "- Takes a single input and gives an output of 0 for negative values and 1 for positive value or zero "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the ReLU fucntion\n",
    "\n",
    "def relu(input):\n",
    "    output = max(0,input)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-3\n"
     ]
    }
   ],
   "source": [
    "# model application using ReLU\n",
    "node_0_input = (input_data * weights[\"node_0\"]).sum()\n",
    "node_0_output = relu(node_0_input)\n",
    "\n",
    "node_1_input = (input_data * weights[\"node_1\"]).sum()\n",
    "node_1_output = relu(node_1_input)\n",
    "\n",
    "hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "\n",
    "model_output = (hidden_layer_outputs * weights[\"output\"]).sum()\n",
    "\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the network to multiple rows of data\n",
    "def predict_with_network(input_data_row, weights):\n",
    "\n",
    "    node_0_input = (input_data_row * weights[\"node_0\"]).sum()\n",
    "    node_0_output = relu(node_0_input)\n",
    "\n",
    "    node_1_input = (input_data_row * weights[\"node_1\"]).sum()\n",
    "    node_1_output = relu(node_1_input)\n",
    "\n",
    "    hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "\n",
    "    input_to_final_layer = (hidden_layer_outputs * weights[\"output\"]).sum()\n",
    "    model_output = relu(input_to_final_layer)\n",
    "\n",
    "    return (model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0, 12]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for input_data_row in input_data:\n",
    "    results.append(predict_with_network(input_data_row, weights))\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "source": [
    "Multi-layer neural networks\n",
    "\n",
    "- forward propagation on multi-layer networks\n",
    "- each layer has two nodes  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = { \"node_0_0\": np.array([3,3]),\n",
    "            \"node_0_1\": np.array([1,5]),\n",
    "            \"node_1_0\": np.array([-1,4]),\n",
    "            \"node_1_1\": np.array([6,-2]),\n",
    "            \"output\": np.array([2,-1])}\n",
    "\n",
    "def predict_with_network(input_data, weights):\n",
    "\n",
    "    # first hidden layer\n",
    "    node_0_0_input = (input_data * weights[\"node_0_0\"]).sum()\n",
    "    node_0_0_output = relu(node_0_0_input)\n",
    "\n",
    "    node_0_1_input = (input_data * weights[\"node_0_1\"]).sum()\n",
    "    node_0_1_ouput = relu(node_0_1_input)\n",
    "\n",
    "    hidden_0_outputs = np.array([node_0_0_output, node_0_1_ouput])\n",
    "\n",
    "    # second hidden layer\n",
    "    node_1_0_input = (hidden_0_outputs * weights[\"node_1_0\"]).sum()\n",
    "    node_1_0_output = relu(node_1_0_input)\n",
    "\n",
    "    node_1_1_input = (hidden_0_outputs * weights[\"node_1_1\"]).sum()\n",
    "    node_1_1_output = relu(node_1_1_input)\n",
    "\n",
    "    hidden_1_outputs = np.array([node_1_0_output, node_1_1_output])\n",
    "\n",
    "    model_output = (hidden_1_outputs * weights[\"output\"]).sum()\n",
    "\n",
    "    return (model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "66\n"
     ]
    }
   ],
   "source": [
    "print(predict_with_network(input_data, weights))"
   ]
  },
  {
   "source": [
    "Optimizing weight changes\n",
    "\n",
    "- changing the weights of the nodes will results in chnges to the loss fucntion as well as the prediction capability of the model\n",
    "- accuracy can be alter with varying the weights"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_network_single(input_data_row, weights):\n",
    "\n",
    "    node_0_input = (input_data_row * weights[\"node_0\"]).sum()\n",
    "    node_0_output = relu(node_0_input)\n",
    "\n",
    "    node_1_input = (input_data_row * weights[\"node_1\"]).sum()\n",
    "    node_1_output = relu(node_1_input)\n",
    "\n",
    "    hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "\n",
    "    input_to_final_layer = (hidden_layer_outputs * weights[\"output\"]).sum()\n",
    "    model_output = relu(input_to_final_layer)\n",
    "\n",
    "    return (model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "6\n0\n"
     ]
    }
   ],
   "source": [
    "input_data = np.array([0,3])\n",
    "\n",
    "weights_0 = {\"node_0\": [2,1],\n",
    "                \"node_1\": [1,2],\n",
    "                \"output\": [1,1]}\n",
    "\n",
    "target_value = 3\n",
    "\n",
    "model_output_0 = predict_with_network_single(input_data, weights_0)\n",
    "\n",
    "error_0 = model_output_0 - target_value\n",
    "\n",
    "# changing the weights to predict better solutions\n",
    "weights_1 = {\"node_0\": [2,1],\n",
    "                \"node_1\": [1,2],\n",
    "                \"output\": [1,0]}\n",
    "\n",
    "model_output_1 = predict_with_network_single(input_data, weights_1)\n",
    "\n",
    "error_1 = model_output_1 - target_value\n",
    "\n",
    "print(error_0)\n",
    "print(error_1)"
   ]
  },
  {
   "source": [
    "Optimizing weight changes for multi-layer networks (multiple data point)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean squared error with weights_0: 40.000000\nMean squared error with weights_1: 11.333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "input_data = np.array([np.array([0,3]), np.array([1,3]), np.array([3,2])])\n",
    "\n",
    "target_actuals = np.array([7,8,5])\n",
    "\n",
    "# Create model_output_0 \n",
    "model_output_0 = []\n",
    "# Create model_output_1\n",
    "model_output_1 = []\n",
    "\n",
    "# Loop over input_data\n",
    "for row in input_data:\n",
    "    # Append prediction to model_output_0\n",
    "    model_output_0.append(predict_with_network_single(row, weights_0))\n",
    "    \n",
    "    # Append prediction to model_output_1\n",
    "    model_output_1.append(predict_with_network_single(row, weights_1))\n",
    "\n",
    "# Calculate the mean squared error for model_output_0: mse_0\n",
    "mse_0 = mean_squared_error(target_actuals, model_output_0)\n",
    "\n",
    "# Calculate the mean squared error for model_output_1: mse_1\n",
    "mse_1 = mean_squared_error(target_actuals, model_output_1)\n",
    "\n",
    "# Print mse_0 and mse_1\n",
    "print(\"Mean squared error with weights_0: %f\" %mse_0)\n",
    "print(\"Mean squared error with weights_1: %f\" %mse_1)"
   ]
  },
  {
   "source": [
    "During optimization the weughts are the values that are being changed and used for getting better predictions\n",
    "\n",
    "- Slopes of the loss func wrt to value  node we feed into\n",
    "- value of the node that feeds into weight\n",
    "- slope of the activation fucntion wrt value we feed into  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# calculate slopes and update the weights\n",
    "weights = np.array([1,2])\n",
    "input_data = np.array([3,4])\n",
    "target = 6\n",
    "learning_rate = 0.01\n",
    "preds =(weights * input_data).sum()\n",
    "error = preds - target\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.5\n"
     ]
    }
   ],
   "source": [
    "gradient = 2 * input_data * error\n",
    "weights_updated = weights - (learning_rate * gradient)\n",
    "preds_updated = (weights_updated * input_data).sum()\n",
    "\n",
    "error_updated = preds_updated - target\n",
    "print(error_updated)"
   ]
  },
  {
   "source": [
    "Back Propagation\n",
    "- start at a random set of weights\n",
    "- use forward propagation to make prediction\n",
    "- use backward propagation to calculate the slope of the loss function wrt each weight\n",
    "- multiply that slope by the learning rate and subtract from the current weightds\n",
    "- keep going with that cycel until we get to flat part"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Stochastic Gradient Descent : Slopes are calculated on one batch at a time\n",
    "- common to calculate the slopes on a subset of data (batch)\n",
    "- use a different batch of data to calculate the next update\n",
    "- start over from beginning once all the data is used\n",
    "- each ruun through the training data is an epoch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Creating a Keras Model\n",
    "* Model building Step\n",
    "    * Specify Architecture\n",
    "    * Compile\n",
    "    * Fit\n",
    "    * Predict    "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import neural network models\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "df = pd.DataFrame(X, columns= iris.feature_names)\n",
    "\n",
    "n_cols = df.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation= \"relu\", input_shape = (n_cols,))) # number of layers from the observations in data\n",
    "model.add(Dense(100, activation= \"relu\"))\n",
    "model.add(Dense(1)) # last layer which is output layer\n",
    "\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model.fit(X, y)"
   ]
  },
  {
   "source": [
    "Categorical data - Classification is the preferred method of analyzing data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df = sns.load_dataset(\"iris\")\n",
    "\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.optimizer_v1 import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience = 2)\n",
    "\n",
    "predictors = iris.data\n",
    "target = to_categorical(iris.target)\n",
    "\n",
    "n_cols = len(iris.feature_names)\n",
    "\n",
    "model_1 = Sequential()\n",
    "model_1.add(Dense(100, activation = \"relu\", input_shape = (n_cols,)))\n",
    "model_1.add(Dense(100, activation = \"relu\"))\n",
    "model_1.add(Dense(100, activation = \"relu\"))\n",
    "model_1.add(Dense(3, activation = \"softmax\"))\n",
    "\n",
    "model_1.compile(optimizer= \"sgd\", loss= \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model_1_training = model_1.fit(predictors, target, validation_split = 0.3, epochs = 10, callbacks = [early_stopping_monitor])\n",
    "model_1_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Sequential()\n",
    "model_2.add(Dense(50, activation = \"relu\", input_shape = (n_cols,)))\n",
    "model_2.add(Dense(50, activation = \"relu\"))\n",
    "model_2.add(Dense(50, activation = \"relu\"))\n",
    "model_2.add(Dense(3, activation = \"softmax\"))\n",
    "\n",
    "model_2.compile(optimizer= \"adam\", loss= \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model_2_training = model_2.fit(predictors, target, validation_split = 0.3, epochs = 10, callbacks = [early_stopping_monitor])\n",
    "model_2_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "plt.plot(model_1_training.history['val_loss'], 'r', model_2_training.history['val_loss'], 'b')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation score')\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "Testing classsification model on Titanic dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code and figure by self"
   ]
  },
  {
   "source": [
    "Once a model is created\n",
    "- save model\n",
    "- reload model\n",
    "- predict using the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "\n",
    "# model.save(\"model_file.h5\")\n",
    "# my_model = load_model(\"model_file.h5\")\n",
    "\n",
    "# predctions = my_model.predict(data_to_predict_with)\n",
    "# probability_true = predictions[:,1]\n",
    "#my_model.summary()"
   ]
  },
  {
   "source": [
    "Stochastic Gradient Descent (in practice)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_new_model():\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(100, activation = \"relu\", input_shape = (n_cols,)))\n",
    "#     model.add(Dense(100, activation = \"relu\"))\n",
    "#     model.add(Dense(2, activation = \"softmax\"))\n",
    "\n",
    "#     return(model)\n",
    "\n",
    "# lr_to_test = [.000001, 0.01, 1]\n",
    "\n",
    "# # loop over the learning rates specified\n",
    "# for lr in lr_to_test:\n",
    "#     model = get_new_model()\n",
    "#     my_optimizer = SGD(lr = lr)\n",
    "#     model.compile(optimizer= my_optimizer, loss = \"categorical_crossentropy\")\n",
    "#     model.fit(predictors, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}